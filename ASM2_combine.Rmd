---
title: "ASM2_combine"
author: "Gp 9"
date: "02/04/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the required libraries
```{r}
library(qdap)
library(dplyr)
library(irlba)
library(e1071)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(biclust)
library(igraph)
library(fpc)
library(quanteda)
```

# Data preparation 
## 1. Load data
We have to be careful with the data since it is encoded in UTF-8. If we do not change the encoding, some symbols are wrongly recognized. 
```{r}
reviews <- read.csv("game_train.csv", encoding = "UTF-8")
games <- read.csv("games.csv", encoding = "UTF-8")
data.kaggle <- read.csv("game_test.csv", encoding = "UTF-8")
```

## 2. Combine data
We combine the reviews with data about the game. We notice that the dataset is relatively balanced so we do not have to over or undersample it. 
```{r}
data <- merge(reviews, games, by.x = "title", by.y = "title", all.x = T)
sum(data$user_suggestion)

data$doc_id <- 1:nrow(data)
train.id <- sample(1:nrow(data), 0.7*nrow(data))
test.id <- data$doc_id[!data$doc_id %in% train.id]
```
```{r, echo=FALSE}
rm(reviews, games)
```

## 3. Clean data

### 3.1 Remove Tags
The Start of the reviews had tags in them which provide further information. These are removed and put into a separate column. 
```{r}
data$is.early <- ifelse(grepl("Early Access Review", data$user_review), 1, 0)
data$user_review <- gsub("Early Access Review","", data$user_review)
data.kaggle$user_review <- gsub("Early Access Review","", data.kaggle$user_review)

data$received.free <- ifelse(grepl("Product received for free", data$user_review), 1, 0)
data$user_review <- gsub("Product received for free","", data$user_review)
data.kaggle$user_review <- gsub("Product received for free","", data.kaggle$user_review)


data$user_review <- gsub("Access Review", "", data$user_review) #unclear, just remove it
data.kaggle$user_review <- gsub("Access Review","", data.kaggle$user_review)
```

```{r}
# Tokenize descriptions
reviewtokens=tokens(data$user_review,what="word",
                    remove_numbers=TRUE,remove_punct=TRUE, remove_symbols=TRUE)
```


```{r}
# Lowercase the tokens
reviewtokens=tokens_tolower(reviewtokens)

term_count <- freq_terms(reviewtokens, 20)
plot(term_count)
```

## remove stop words 
```{r}
reviewtokens=tokens_select(reviewtokens, stopwords("english"), selection = "remove")

term_count <- freq_terms(reviewtokens, 20)
plot(term_count)
```


#remoce unnecessary words
```{r}
rmwords <- c("game", "can", "get", "play", "games", "playing", "really", "now", "people", "much", "also", "played", "players", "lot", "many", "hours", "just", "time", "one", "i", "im", "even", "on", "player", "because", "etc",
             "â–‘", "$", "+", "-", ">", "<", "!", "@", "#", "?")

reviewtokens=tokens_remove(reviewtokens,rmwords)
term_count <- freq_terms(reviewtokens, 20)
plot(term_count)
```

```{r}
# Stemming tokens
reviewtokens=tokens_wordstem(reviewtokens,language = "english")
reviewtokens=tokens_ngrams(reviewtokens,n=1:2)

```

```{r}
# Creating a bag of words
reviewtokensdfm=dfm(reviewtokens,tolower = FALSE)

# Remove sparsity
reviewSparse <- convert(reviewtokensdfm, "tm")
tm::removeSparseTerms(reviewSparse, 0.7)

# Create the dfm
dfm_trim(reviewtokensdfm, min_docfreq = 0.3)
x=dfm_trim(reviewtokensdfm, sparsity = 0.98)
```

```{r}
## Setup a dataframe with features
df=convert(x,to="data.frame")

##Add the Y variable Recommend.IND
reviewtokensdf=cbind(data$user_suggestion,df)
head(reviewtokensdf)

## Cleanup names
names(reviewtokensdf)[names(reviewtokensdf) == "data$user_suggestion"] <- "suggestion"

#names(reviewtokensdf)=make.names(names(reviewtokensdf))
head(reviewtokensdf)

## Remove the original review.text column
reviewtokensdf=reviewtokensdf[,-c(2)]
head(reviewtokensdf)
reviewtokensdf$suggestion=factor(reviewtokensdf$suggestion)

```

```{r}
###Split into train and test
split1<- sample(c(rep(0, 0.7 * nrow(reviewtokensdf)), rep(1, 0.3 * nrow(reviewtokensdf))))
train <- reviewtokensdf[split1 == 0, ]        
test <- reviewtokensdf[split1== 1, ]    
```


## Build the CART model
```{r}
tree=rpart(formula = suggestion ~ ., data = train, method="class",control = rpart.control(minsplit = 200,  minbucket = 30, cp = 0.0001))

printcp(tree)
plotcp(tree)

##Prune down the tree
bestcp=tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
bestcp
ptree=prune(tree,cp=bestcp)
rpart.plot(ptree,cex = 0.6)
prp(ptree, faclen = 0, cex = 0.5, extra = 2)
```

###Prediction
```{r}
actual_class <- test$suggestion
predicted_class <- predict(tree, newdata = test)
tab_class <- table(actual_class, predicted_class)
tab_class

confusionMatrix(tab_class, mode = "everything")
```

## Build the Random Forest model
```{r}
library(randomForest)
reviewRF=randomForest(suggestion~., data=train)
varImpPlot(reviewRF, cex=.7)
```

###Prediction
```{r}
actual_class <- test$suggestion
predicted_class <- predict(reviewRF, newdata = test)
tab_class <- table(actual_class, predicted_class)
tab_class

confusionMatrix(tab_class, mode = "everything")

```

##LASSO
```{r}
library(glmnet)
#convert training data to matrix format
x <- model.matrix(suggestion~.,train)
#convert class to numerical variable
y <- as.numeric(train$suggestion)
#perform grid search to find optimal value of lambda
cv.out <- cv.glmnet(x,y,alpha=1,family="binomial",type.measure = "mse" )
#plot result
plot(cv.out)

#min value of lambda
lambda_min <- cv.out$lambda.min
#best value of lambda
lambda_1se <- cv.out$lambda.1se
lambda_1se
#regression coefficients
coef=coef(cv.out,s=lambda_1se)
lassocoef=as.matrix(coef(cv.out,s=lambda_1se))

control <- rpart.control(minsplit = 20,
                         minbucket = round(100 / 3),
                         maxdepth = 30,
                         cp = 0.01)

tune_fit <- rpart(suggestion~., data=train, method = 'class', control = control)
```

###Prediction
```{r}
predict_1 <-predict(tune_fit, newdata = test, type = 'class')
tab_class <- table(actual_class, predict_1)
tab_class
confusionMatrix(tab_class, mode = "everything")
```

##Decision tress
```{r}
tree.model <- rpart(suggestion~., data=train, method = 'class')
```

###Prediction
```{r}
actual_class <- test$suggestion
predicted_class <- predict(tree.model, newdata = test, type = "class")
tab_class <- table(actual_class, predicted_class)
tab_class

confusionMatrix(tab_class, mode = "everything")

```

##Naive bayes
```{r}
naive.bayes <- naiveBayes(suggestion~., data=train, distribution = "multinomial", 
                            prior = "termfreq")
```

###Prediction
```{r}
predict.naive = predict(naive.bayes, newdata = test)
actual_class <- test$suggestion
tab_class <- table(actual_class, predict.naive)
tab_class

confusionMatrix(tab_class, mode = "everything")
```

##SVM
```{r}
svm.model = svm(formula = suggestion ~ .,
                data = train,
                type = 'C-classification',
                kernel = 'linear')
```

###Prediction
```{r}
actual_class <- test$suggestion
predicted_class <- predict(fit.svm, newdata = test)
tab_class <- table(actual_class, predicted_class)
tab_class

confusionMatrix(tab_class, mode = "everything")
```

```{r}
library(caret)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"

# a) linear algorithms
set.seed(7)
fit.lda <- train(suggestion~., data=train, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(suggestion~., data=data.frame(train), method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(suggestion~., data=train, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(suggestion~., data=train, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(suggestion~., data=train, method="rf", metric=metric, trControl=control)
```

```{r}
#summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
```

