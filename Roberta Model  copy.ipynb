{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RoBERTa model for the Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rn\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we only use the user_review variable to predict the sentiment, and see whether the sentiment could accurately predict the user's suggestion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"game_train.csv\")\n",
    "df_test = pd.read_csv(\"game_test.csv\")\n",
    "\n",
    "# convert review text to string\n",
    "df_reviews[\"user_review\"] = df_reviews[\"user_review\"].astype(str)\n",
    "df_reviews.user_review = df_reviews.user_review.apply(lambda s: s.strip())\n",
    "\n",
    "df_test[\"user_review\"] = df_test[\"user_review\"].astype(str)\n",
    "df_test.user_review = df_test.user_review.apply(lambda s: s.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5986\n",
       "0    4508\n",
       "Name: user_suggestion, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[\"user_suggestion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply data cleaning: we remove the early access review comments and remove duplicated rows. We also figure out the foul language in the review was replaced by ♥ emoji. So to increase the accurancy of the sentiment prediction, we replaced ♥ with **, as the model would consider ** as foul language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10494, 5)\n",
      "(10494, 5)\n"
     ]
    }
   ],
   "source": [
    "#Remove the \"Early Access Review\" comments\n",
    "\n",
    "df_reviews_2 = df_reviews[df_reviews.user_review != \"Early Access Review\"]\n",
    "df_reviews_2 = df_reviews[~df_reviews.user_review.isin(['nan'])]\n",
    "print(df_reviews_2.shape)\n",
    "\n",
    "# Drop duplicates \n",
    "df_reviews_2.drop_duplicates(['user_review', 'user_suggestion'], inplace = True)\n",
    "print(df_reviews_2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I'm scared and hearing creepy voices.  So I'll...\n",
       "1    Best game, more better than Sam Pepper's YouTu...\n",
       "2    A littly iffy on the controls, but once you kn...\n",
       "3    Great game, fun and colorful and all that.A si...\n",
       "4    Early Access ReviewIt's pretty cute at first, ...\n",
       "Name: user_review_clean, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace ♥\n",
    "def replace_hearts_with_PAD(text):\n",
    "    return re.sub(r\"[♥]+\", ' **** ' ,text)\n",
    "\n",
    "df_reviews_2['user_review_clean'] = df_reviews_2.user_review.apply(replace_hearts_with_PAD)\n",
    "\n",
    "df_reviews_3 = df_reviews_2[['user_review_clean', 'user_suggestion']]\n",
    "df_reviews_3 = df_reviews_3.rename({\"user_review_clean\": \"text\", \"user_suggestion\": \"labels\"});\n",
    "df_reviews_3.head()\n",
    "\n",
    "\n",
    "\n",
    "df_test['user_review_clean'] = df_test.user_review.apply(replace_hearts_with_PAD)\n",
    "df_test_1 = df_test['user_review_clean']\n",
    "df_test_1 = df_test_1.rename({\"user_review_clean\":\"text\"})\n",
    "df_test_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the df_training into train (60%), test(20%) and holdout sets(20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6296, 2)\n",
      "(2099, 2)\n",
      "(2099, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df, eval_df = train_test_split(df_reviews_3, test_size = 0.4, random_state = 42)\n",
    "test_df , holdout_df = train_test_split(eval_df, test_size = 0.5, random_state = 42)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(holdout_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta model: Roberta model means robustly Optimized BERT Pre-training Approach, we use simpletransformers package to create the model. And it is recommended to use num_train_epochs=1 and for loop to repeat the training, as using for loop could get the same result saw on the epoch, but if set num_train_epochs more than 1, we could not get the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:585: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 0/6296 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/6296 [00:26<3:33:20,  2.04s/it]\n",
      "/opt/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epochs 0/1. Running Loss:    0.2142: 100%|██████████| 394/394 [7:02:36<00:00, 64.36s/it]\n",
      "Epoch 1 of 1: 100%|██████████| 1/1 [7:02:36<00:00, 25356.80s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/.\n",
      "/opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:1426: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 0/2099 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2099 [00:15<1:44:54,  3.01s/it]\n",
      "Running Evaluation: 100%|██████████| 132/132 [12:30<00:00,  5.69s/it]\n",
      "INFO:simpletransformers.classification.classification_model:{'mcc': 0.7799084084321967, 'tp': 1092, 'tn': 781, 'fp': 120, 'fn': 106, 'auroc': 0.9555733844235398, 'auprc': 0.9661060437059306, 'eval_loss': 0.2844462984551986}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.8923296808003811\n",
      "Recall =  0.8804960541149943\n",
      "{'mcc': 0.7799084084321967, 'tp': 1092, 'tn': 781, 'fp': 120, 'fn': 106, 'auroc': 0.9555733844235398, 'auprc': 0.9661060437059306, 'eval_loss': 0.2844462984551986}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'user_review'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/Roberta Model  copy.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/Roberta%20Model%20%20copy.ipynb#ch0000011?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecall = \u001b[39m\u001b[39m\"\u001b[39m,(result[\u001b[39m'\u001b[39m\u001b[39mtn\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m/\u001b[39m (result[\u001b[39m'\u001b[39m\u001b[39mtn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39mfn\u001b[39m\u001b[39m'\u001b[39m])) \u001b[39m# simpletransformers mistakenly reports fn and fp. have to flip them\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/Roberta%20Model%20%20copy.ipynb#ch0000011?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/Roberta%20Model%20%20copy.ipynb#ch0000011?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(np\u001b[39m.\u001b[39margmax(model_outputs, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m), test_df\u001b[39m.\u001b[39;49muser_review\u001b[39m.\u001b[39mvalues))\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py:5583\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5575'>5576</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5576'>5577</a>\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5577'>5578</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5578'>5579</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5579'>5580</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5580'>5581</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5581'>5582</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5582'>5583</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'user_review'"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "roberta_model = ClassificationModel(\n",
    "                          'roberta', 'roberta-base', use_cuda=False,\n",
    "                          args={'num_train_epochs' : 1,\n",
    "                                 \"train_batch_size\": 16,\n",
    "                                 \"eval_batch_size\": 16,\n",
    "                                 \"fp16\": False,\n",
    "                                 \"optimizer\": \"AdamW\",\n",
    "                                 \"adam_epsilon\": 1e-8,\n",
    "                                 \"learning_rate\": 1e-5,\n",
    "                                 \"weight_decay\": 0.7,\n",
    "                                 'overwrite_output_dir': True,\n",
    "                                 \"save_eval_checkpoints\": False,\n",
    "                                 \"save_model_every_epoch\": False,\n",
    "                                 \"no_cache\": True,\n",
    "                                 \"manual_seed\": 12345})\n",
    "\n",
    "for i in range(2):\n",
    "     # Train the model\n",
    "    roberta_model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model on the test data \n",
    "    result, model_outputs, wrong_predictions = roberta_model.eval_model(test_df)\n",
    "    print(\"Accuracy= \" ,(result['tp'] + result['tn']) / (result['tp'] + result['tn'] + \\\n",
    "                                                         result['fp'] + result['fn']))\n",
    "    print(\"Recall = \",(result['tn']) / (result['tn'] + result['fn'])) # simpletransformers mistakenly reports fn and fp. have to flip them\n",
    "    print(result)\n",
    "    print(classification_report(np.argmax(model_outputs, axis = 1), test_df.user_review_clean.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO:simpletransformers.classification.classification_model:{'mcc': 0.7799084084321967, 'tp': 1092, 'tn': 781, 'fp': 120, 'fn': 106, 'auroc': 0.9555733844235398, 'auprc': 0.9661060437059306, 'eval_loss': 0.2844462984551986}\n",
    "Accuracy=  0.8923296808003811\n",
    "Recall =  0.8804960541149943\n",
    "f1 score = 0.90622\n",
    "{'mcc': 0.7799084084321967, 'tp': 1092, 'tn': 781, 'fp': 120, 'fn': 106, 'auroc': 0.9555733844235398, 'auprc': 0.9661060437059306, 'eval_loss': 0.2844462984551986}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 0/6996 [00:14<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "y_test_id = df_test['review_id']\n",
    "y_test = roberta_model.predict(df_test_1)\n",
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test = y_test.assign(review_id = y_test_id)\n",
    "y_test.columns = ['review_id','user_suggestion']\n",
    "pd.DataFrame(y_test).to_csv('predictions.kaggle.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e265db2db6eeaa7b75fb14568ce315d5b55c8813863eb0c0ad2b3eadd78192ec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
