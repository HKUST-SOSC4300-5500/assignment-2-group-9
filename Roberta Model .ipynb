{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RoBERTa model for the Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rn\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we only use the user_review variable to predict the sentiment, and see whether the sentiment could accurately predict the user's suggestion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"game_train.csv\")\n",
    "df_test = pd.read_csv(\"game_test.csv\")\n",
    "\n",
    "# convert review text to string\n",
    "df_reviews[\"user_review\"] = df_reviews[\"user_review\"].astype(str)\n",
    "df_reviews.user_review = df_reviews.user_review.apply(lambda s: s.strip())\n",
    "\n",
    "df_test[\"user_review\"] = df_test[\"user_review\"].astype(str)\n",
    "df_test.user_review = df_test.user_review.apply(lambda s: s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5986\n",
       "0    4508\n",
       "Name: user_suggestion, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[\"user_suggestion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply data cleaning: we remove the early access review comments and remove duplicated rows. We also figure out the foul language in the review was replaced by ♥ emoji. So to increase the accurancy of the sentiment prediction, we replaced ♥ with **, as the model would consider ** as foul language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10494, 5)\n",
      "(10494, 5)\n"
     ]
    }
   ],
   "source": [
    "#Remove the \"Early Access Review\" comments\n",
    "\n",
    "df_reviews_2 = df_reviews[df_reviews.user_review != \"Early Access Review\"]\n",
    "df_reviews_2 = df_reviews[~df_reviews.user_review.isin(['nan'])]\n",
    "print(df_reviews_2.shape)\n",
    "\n",
    "# Drop duplicates \n",
    "df_reviews_2.drop_duplicates(['user_review', 'user_suggestion'], inplace = True)\n",
    "print(df_reviews_2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ♥\n",
    "def replace_hearts_with_PAD(text):\n",
    "    return re.sub(r\"[♥]+\", ' **** ' ,text)\n",
    "\n",
    "df_reviews_2['user_review_clean'] = df_reviews_2.user_review.apply(replace_hearts_with_PAD)\n",
    "\n",
    "df_reviews_3 = df_reviews_2[['user_review_clean', 'user_suggestion']]\n",
    "df_reviews_3 = df_reviews_3.rename({\"user_review_clean\": \"text\", \"user_suggestion\": \"labels\"});\n",
    "df_reviews_3.head()\n",
    "\n",
    "df_test_1 = df_test['user_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the df_training into train (60%), test(20%) and holdout sets(20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6296, 2)\n",
      "(2099, 2)\n",
      "(2099, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df, eval_df = train_test_split(df_reviews_3, test_size = 0.4, random_state = 42)\n",
    "test_df , holdout_df = train_test_split(eval_df, test_size = 0.5, random_state = 42)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(holdout_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta model: Roberta model means robustly Optimized BERT Pre-training Approach, we use simpletransformers package to create the model. And it is recommended to use num_train_epochs=1 and for loop to repeat the training, as using for loop could get the same result saw on the epoch, but if set num_train_epochs more than 1, we could not get the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:585: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 0/6296 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/6296 [00:25<3:26:41,  1.97s/it]\n",
      "/opt/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epochs 0/1. Running Loss:    0.3277:  18%|█▊        | 72/394 [5:35:30<25:00:28, 279.59s/it]\n",
      "Epoch 1 of 1:   0%|          | 0/1 [5:35:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2 .ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=10'>11</a>\u001b[0m roberta_model \u001b[39m=\u001b[39m ClassificationModel(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=11'>12</a>\u001b[0m                           \u001b[39m'\u001b[39m\u001b[39mroberta\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m'\u001b[39m, use_cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=12'>13</a>\u001b[0m                           args\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mnum_train_epochs\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=23'>24</a>\u001b[0m                                  \u001b[39m\"\u001b[39m\u001b[39mno_cache\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=24'>25</a>\u001b[0m                                  \u001b[39m\"\u001b[39m\u001b[39mmanual_seed\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m12345\u001b[39m})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=27'>28</a>\u001b[0m      \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=28'>29</a>\u001b[0m     roberta_model\u001b[39m.\u001b[39;49mtrain_model(train_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=30'>31</a>\u001b[0m \u001b[39m# Evaluate the model on the test data \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chungyuetman/Desktop/SOSC4300/assignment-2-group-9-main/asm2%20.ipynb#ch0000009?line=31'>32</a>\u001b[0m     result, model_outputs, wrong_predictions \u001b[39m=\u001b[39m roberta_model\u001b[39m.\u001b[39meval_model(test_df)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:605\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=595'>596</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=596'>597</a>\u001b[0m     train_dataset,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=597'>598</a>\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=598'>599</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=599'>600</a>\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=600'>601</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=602'>603</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=604'>605</a>\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=605'>606</a>\u001b[0m     train_dataloader,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=606'>607</a>\u001b[0m     output_dir,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=607'>608</a>\u001b[0m     multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=608'>609</a>\u001b[0m     show_running_loss\u001b[39m=\u001b[39;49mshow_running_loss,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=609'>610</a>\u001b[0m     eval_df\u001b[39m=\u001b[39;49meval_df,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=610'>611</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=611'>612</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=612'>613</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=614'>615</a>\u001b[0m \u001b[39m# model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=615'>616</a>\u001b[0m \u001b[39m# model_to_save.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=616'>617</a>\u001b[0m \u001b[39m# self.tokenizer.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=617'>618</a>\u001b[0m \u001b[39m# torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=618'>619</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:914\u001b[0m, in \u001b[0;36mClassificationModel.train\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=911'>912</a>\u001b[0m     scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=912'>913</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=913'>914</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=915'>916</a>\u001b[0m tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py?line=916'>917</a>\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "roberta_model = ClassificationModel(\n",
    "                          'roberta', 'roberta-base', use_cuda=False,\n",
    "                          args={'num_train_epochs' : 1,\n",
    "                                 \"train_batch_size\": 16,\n",
    "                                 \"eval_batch_size\": 16,\n",
    "                                 \"fp16\": False,\n",
    "                                 \"optimizer\": \"AdamW\",\n",
    "                                 \"adam_epsilon\": 1e-8,\n",
    "                                 \"learning_rate\": 1e-5,\n",
    "                                 \"weight_decay\": 0.7,\n",
    "                                 'overwrite_output_dir': True,\n",
    "                                 \"save_eval_checkpoints\": False,\n",
    "                                 \"save_model_every_epoch\": False,\n",
    "                                 \"no_cache\": True,\n",
    "                                 \"manual_seed\": 12345})\n",
    "\n",
    "for i in range(2):\n",
    "     # Train the model\n",
    "    roberta_model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model on the test data \n",
    "    result, model_outputs, wrong_predictions = roberta_model.eval_model(test_df)\n",
    "    print(\"Accuracy= \" ,(result['tp'] + result['tn']) / (result['tp'] + result['tn'] + \\\n",
    "                                                         result['fp'] + result['fn']))\n",
    "    print(\"Recall = \",(result['tn']) / (result['tn'] + result['fn'])) # simpletransformers mistakenly reports fn and fp. have to flip them\n",
    "    print(result)\n",
    "    print(classification_report(np.argmax(model_outputs, axis = 1), test_df.user_review.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO:simpletransformers.classification.classification_model:{'mcc': 0.7799084084321967, 'tp': 1092, 'tn': 781, 'fp': 120, 'fn': 106, 'auroc': 0.9555733844235398, 'auprc': 0.9661060437059306, 'eval_loss': 0.2844462984551986}\n",
    "Accuracy=  0.8923296808003811\n",
    "Recall =  0.8804960541149943\n",
    "f1 score = 0.90622\n",
    "{'mcc': 0.7799084084321967, 'tp': 1092, 'tn': 781, 'fp': 120, 'fn': 106, 'auroc': 0.9555733844235398, 'auprc': 0.9661060437059306, 'eval_loss': 0.2844462984551986}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 0/6996 [00:14<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "y_test_id = df_test['review_id']\n",
    "y_test = roberta_model.predict(df_test_1)\n",
    "\n",
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test = y_test.assign(review_id = y_test_id)\n",
    "y_test.columns = ['review_id','user_suggestion']\n",
    "pd.DataFrame(y_test).to_csv('predictions.kaggle.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e265db2db6eeaa7b75fb14568ce315d5b55c8813863eb0c0ad2b3eadd78192ec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
